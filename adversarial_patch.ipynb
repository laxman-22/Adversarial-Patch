{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries\n",
    "\n",
    "Note: The cell below contains code from Tutorial  10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional required setup steps \n",
    "\n",
    "Note: The cell below contains code from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"./data\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the ImageNet Dataset and process the zip files to ensure they are ready to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "import zipfile\n",
    "# Github URL where the dataset is stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/\"\n",
    "# Files to download\n",
    "dataset = [(DATASET_PATH, \"TinyImageNet.zip\")]\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for dir_name, file_name in dataset:\n",
    "    file_path = os.path.join(dir_name, file_name)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
    "        if file_name.endswith(\".zip\"):\n",
    "            print(\"Unzipping file...\")\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(file_path.rsplit(\"/\",1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Pretrained ResNet34 model from Pytorch\n",
    "\n",
    "Note: The code in the cell below is from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNN architecture pretrained on ImageNet\n",
    "os.environ[\"TORCH_HOME\"] = './model'\n",
    "pretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "retrained_model = pretrained_model.to(device)\n",
    "\n",
    "# No gradients needed for the network\n",
    "pretrained_model.eval()\n",
    "for p in pretrained_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definition to map patch parameters into image value ranges\n",
    "\n",
    "Note: The code below is from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "NORM_STD = np.array([0.229, 0.224, 0.225])\n",
    "TENSOR_MEANS, TENSOR_STD = torch.FloatTensor(NORM_MEAN)[:,None,None], torch.FloatTensor(NORM_STD)[:,None,None]\n",
    "def patch_forward(patch):\n",
    "    # Map patch values from [-infty,infty] to ImageNet min and max\n",
    "    patch = (torch.tanh(patch) + 1 - 2 * TENSOR_MEANS) / (2 * TENSOR_STD)\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define random patch placement function to train the new patches on\n",
    "\n",
    "Note: The code below is from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_patch(img, patch):\n",
    "    for i in range(img.shape[0]):\n",
    "        h_offset = np.random.randint(0,img.shape[2]-patch.shape[1]-1)\n",
    "        w_offset = np.random.randint(0,img.shape[3]-patch.shape[2]-1)\n",
    "        img[i,:,h_offset:h_offset+patch.shape[1],w_offset:w_offset+patch.shape[2]] = patch_forward(patch)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function to analyze performance of patch\n",
    "\n",
    "Note: The code below is from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_patch(model, patch, val_loader, target_class):\n",
    "    model.eval()\n",
    "    tp, tp_5, counter = 0., 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
    "            # For stability, place the patch at 4 random locations per image, and average the performance\n",
    "            for _ in range(4):\n",
    "                patch_img = place_patch(img, patch)\n",
    "                patch_img = patch_img.to(device)\n",
    "                img_labels = img_labels.to(device)\n",
    "                pred = model(patch_img)\n",
    "                # In the accuracy calculation, we need to exclude the images that are of our target class\n",
    "                # as we would not \"fool\" the model into predicting those\n",
    "                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n",
    "                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n",
    "                counter += (img_labels != target_class).sum()\n",
    "    acc = tp/counter\n",
    "    top5 = tp_5/counter\n",
    "    return acc, top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop for patch training\n",
    "\n",
    "Note: The code below is from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_attack(model, target_class, patch_size=64, num_epochs=5):\n",
    "    # Leave a small set of images out to check generalization\n",
    "    # In most of our experiments, the performance on the hold-out data points\n",
    "    # was as good as on the training set. Overfitting was little possible due\n",
    "    # to the small size of the patches.\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [4500, 500])\n",
    "    train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True, num_workers=8)\n",
    "    val_loader = data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "    # Create parameter and optimizer\n",
    "    if not isinstance(patch_size, tuple):\n",
    "        patch_size = (patch_size, patch_size)\n",
    "    patch = nn.Parameter(torch.zeros(3, patch_size[0], patch_size[1]), requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        t = tqdm(train_loader, leave=False)\n",
    "        for img, _ in t:\n",
    "            img = place_patch(img, patch)\n",
    "            img = img.to(device)\n",
    "            pred = model(img)\n",
    "            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n",
    "            loss = loss_module(pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
    "\n",
    "    # Final validation\n",
    "    acc, top5 = eval_patch(model, patch, val_loader, target_class)\n",
    "\n",
    "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results for new patches that are created\n",
    "\n",
    "Note: The code below is from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_results = {}\n",
    "def save_results(patch_dict):\n",
    "    result_dict = {cname: {psize: [t.item() if isinstance(t, torch.Tensor) else t\n",
    "                                   for t in patch_dict[cname][psize][\"results\"]]\n",
    "                           for psize in patch_dict[cname]}\n",
    "                   for cname in patch_dict}\n",
    "    with open(os.path.join('./model', \"patch_results.json\"), \"w\") as f:\n",
    "        json.dump(result_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the dataset\n",
    "\n",
    "Note: The code below is from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No resizing and center crop necessary as images are already preprocessed.\n",
    "plain_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN,\n",
    "                         std=NORM_STD)\n",
    "])\n",
    "\n",
    "# Load dataset and create data loader\n",
    "imagenet_path = os.path.join(DATASET_PATH, \"TinyImageNet/\")\n",
    "assert os.path.isdir(imagenet_path), f\"Could not find the ImageNet dataset at expected path \\\"{imagenet_path}\\\". \" + \\\n",
    "                                     f\"Please make sure to have downloaded the ImageNet dataset here, or change the {DATASET_PATH=} variable.\"\n",
    "dataset = torchvision.datasets.ImageFolder(root=imagenet_path, transform=plain_transforms)\n",
    "data_loader = data.DataLoader(dataset, batch_size=32, shuffle=False, drop_last=False, num_workers=8)\n",
    "\n",
    "# Load label names to interpret the label numbers 0 to 999\n",
    "with open(os.path.join(imagenet_path, \"label_list.json\"), \"r\") as f:\n",
    "    label_names = json.load(f)\n",
    "\n",
    "def get_label_index(lab_str):\n",
    "    assert lab_str in label_names, f\"Label \\\"{lab_str}\\\" not found. Check the spelling of the class.\"\n",
    "    return label_names.index(lab_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definition for training the model\n",
    "\n",
    "Note: The code below is from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(class_names, patch_sizes):\n",
    "    result_dict = dict()\n",
    "\n",
    "    # Loop over all classes and patch sizes\n",
    "    for name in class_names:\n",
    "        result_dict[name] = dict()\n",
    "        for patch_size in patch_sizes:\n",
    "            c = label_names.index(name)\n",
    "            file_name = os.path.join('./model', f\"{name}_{patch_size}_patch.pt\")\n",
    "            # Load patch if pretrained file exists, otherwise start training\n",
    "            if not os.path.isfile(file_name):\n",
    "                patch, val_results = patch_attack(pretrained_model, target_class=c, patch_size=patch_size, num_epochs=5)\n",
    "                print(f\"Validation results for {name} and {patch_size}:\", val_results)\n",
    "                torch.save(patch, file_name)\n",
    "            else:\n",
    "                patch = torch.load(file_name)\n",
    "            # Load evaluation results if exist, otherwise manually evaluate the patch\n",
    "            if name in json_results:\n",
    "                results = json_results[name][str(patch_size)]\n",
    "            else:\n",
    "                results = eval_patch(pretrained_model, patch, data_loader, target_class=c)\n",
    "\n",
    "            # Store results and the patches in a dict for better access\n",
    "            result_dict[name][patch_size] = {\n",
    "                \"results\": results,\n",
    "                \"patch\": patch\n",
    "            }\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model\n",
    "\n",
    "Note: A portion of this code is from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['dam', 'fly']\n",
    "patch_sizes = [128]\n",
    "\n",
    "patch_dict = get_patches(class_names, patch_sizes)\n",
    "save_results(patch_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the patches\n",
    "\n",
    "Note: The code below is from Tutorial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_patches():\n",
    "    fig, ax = plt.subplots(len(patch_sizes), len(class_names), figsize=(len(class_names)*2.2, len(patch_sizes)*2.2))\n",
    "    ax = np.atleast_2d(ax)\n",
    "    for c_idx, cname in enumerate(class_names):\n",
    "        for p_idx, psize in enumerate(patch_sizes):\n",
    "            patch = patch_dict[cname][psize][\"patch\"]\n",
    "            patch = (torch.tanh(patch) + 1) / 2 # Parameter to pixel values\n",
    "            patch = patch.cpu().permute(1, 2, 0).numpy()\n",
    "            patch = np.clip(patch, a_min=0.0, a_max=1.0)\n",
    "            ax[p_idx][c_idx].imshow(patch)\n",
    "            ax[p_idx][c_idx].set_title(f\"{cname}, size {psize}\")\n",
    "            ax[p_idx][c_idx].axis('off')\n",
    "            patch_image = (patch * 255).astype(np.uint8) \n",
    "            patch_image_pil = Image.fromarray(patch_image)\n",
    "            patch_image_pil.save(f\"{cname}_size_{psize}_patch.png\")\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    plt.show()\n",
    "show_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the two generated images to create a new patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image1 = Image.open('dam_size_128_patch.png')\n",
    "image2 = Image.open('fly_size_128_patch.png')\n",
    "new_image = Image.new('RGB', image1.size)\n",
    "\n",
    "pixels1 = image1.load()\n",
    "pixels2 = image2.load()\n",
    "new_pixels = new_image.load()\n",
    "\n",
    "width, height = image1.size\n",
    "\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        pixel1 = pixels1[x, y]\n",
    "        pixel2 = pixels2[x, y]\n",
    "        new_pixel = (\n",
    "            (pixel1[0] + pixel2[0]) // 2,\n",
    "            (pixel1[1] + pixel2[1]) // 2, \n",
    "            (pixel1[2] + pixel2[2]) // 2\n",
    "        )\n",
    "        new_pixels[x, y] = new_pixel\n",
    "\n",
    "new_image.save('merged_image.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
